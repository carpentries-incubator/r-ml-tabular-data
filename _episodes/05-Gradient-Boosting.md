---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 05-Gradient-Boosting.md in _episodes_rmd/
source: Rmd
title: "Gradient Boosted Trees"
teaching: 50 
exercises: 15
questions:
- "What is gradient boosting?"
- "TODO"
objectives:
- "TODO"
keypoints:
- "TODO"
---



## Gradient Boosted Trees

TODO: 


## Reload the red wine data


~~~
library(tidyverse)
~~~
{: .language-r}



~~~
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
~~~
{: .output}



~~~
✔ ggplot2 3.3.5     ✔ purrr   0.3.4
✔ tibble  3.1.6     ✔ dplyr   1.0.8
✔ tidyr   1.2.0     ✔ stringr 1.4.0
✔ readr   2.1.2     ✔ forcats 0.5.1
~~~
{: .output}



~~~
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
~~~
{: .output}



~~~
library(here)
~~~
{: .language-r}



~~~
here() starts at /home/runner/work/r-ml-tabular-data/r-ml-tabular-data
~~~
{: .output}



~~~
wine <- read_csv(here("data", "wine.csv"))
~~~
{: .language-r}



~~~
Rows: 6497 Columns: 12
~~~
{: .output}



~~~
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (12): fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlo...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
~~~
{: .output}



~~~
redwineR <- wine %>% slice(1:1599) 
trainSize <- round(0.80 * nrow(redwineR))
set.seed(1234) 
trainIndex <- sample(nrow(redwineR), trainSize)
trainDF <- redwineR %>% slice(trainIndex)
testDF <- redwineR %>% slice(-trainIndex)
~~~
{: .language-r}

## Regression Model


~~~
library(xgboost)
~~~
{: .language-r}



~~~

Attaching package: 'xgboost'
~~~
{: .output}



~~~
The following object is masked from 'package:dplyr':

    slice
~~~
{: .output}



~~~
gbm <- xgboost(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality, nrounds = 50)
~~~
{: .language-r}



~~~
[1]	train-rmse:3.676364 
[2]	train-rmse:2.617381 
[3]	train-rmse:1.884501 
[4]	train-rmse:1.380320 
[5]	train-rmse:1.040359 
[6]	train-rmse:0.812738 
[7]	train-rmse:0.661508 
[8]	train-rmse:0.563880 
[9]	train-rmse:0.499910 
[10]	train-rmse:0.455213 
[11]	train-rmse:0.427036 
[12]	train-rmse:0.399291 
[13]	train-rmse:0.388654 
[14]	train-rmse:0.368818 
[15]	train-rmse:0.361202 
[16]	train-rmse:0.347489 
[17]	train-rmse:0.333721 
[18]	train-rmse:0.327205 
[19]	train-rmse:0.322311 
[20]	train-rmse:0.311274 
[21]	train-rmse:0.306126 
[22]	train-rmse:0.304587 
[23]	train-rmse:0.302227 
[24]	train-rmse:0.291819 
[25]	train-rmse:0.283164 
[26]	train-rmse:0.274758 
[27]	train-rmse:0.263478 
[28]	train-rmse:0.261211 
[29]	train-rmse:0.252939 
[30]	train-rmse:0.242802 
[31]	train-rmse:0.235155 
[32]	train-rmse:0.223787 
[33]	train-rmse:0.215584 
[34]	train-rmse:0.210648 
[35]	train-rmse:0.207097 
[36]	train-rmse:0.205686 
[37]	train-rmse:0.203362 
[38]	train-rmse:0.195475 
[39]	train-rmse:0.192458 
[40]	train-rmse:0.188874 
[41]	train-rmse:0.183996 
[42]	train-rmse:0.178708 
[43]	train-rmse:0.170608 
[44]	train-rmse:0.165917 
[45]	train-rmse:0.158432 
[46]	train-rmse:0.152803 
[47]	train-rmse:0.149589 
[48]	train-rmse:0.147823 
[49]	train-rmse:0.147145 
[50]	train-rmse:0.141775 
~~~
{: .output}


~~~
pQuality <- predict(gbm, as.matrix(select(testDF, -quality)))
gbRMSE <- sqrt(mean((pQuality - testDF$quality)^2))
gbRMSE
~~~
{: .language-r}



~~~
[1] 0.6489389
~~~
{: .output}

## More Details on the Training Process


~~~
dtrain <- xgb.DMatrix(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality)
dtest <- xgb.DMatrix(data = as.matrix(select(testDF, -quality)), label = testDF$quality)
watch <- list(train = dtrain, test = dtest)
~~~
{: .language-r}


~~~
gbm <- xgb.train(data = dtrain, watchlist = watch, nrounds = 50)
~~~
{: .language-r}



~~~
[1]	train-rmse:3.676364	test-rmse:3.676491 
[2]	train-rmse:2.617381	test-rmse:2.620643 
[3]	train-rmse:1.884501	test-rmse:1.904736 
[4]	train-rmse:1.380320	test-rmse:1.414777 
[5]	train-rmse:1.040359	test-rmse:1.110905 
[6]	train-rmse:0.812738	test-rmse:0.913236 
[7]	train-rmse:0.661508	test-rmse:0.793578 
[8]	train-rmse:0.563880	test-rmse:0.729415 
[9]	train-rmse:0.499910	test-rmse:0.686777 
[10]	train-rmse:0.455213	test-rmse:0.665393 
[11]	train-rmse:0.427036	test-rmse:0.650133 
[12]	train-rmse:0.399291	test-rmse:0.642655 
[13]	train-rmse:0.388654	test-rmse:0.640231 
[14]	train-rmse:0.368818	test-rmse:0.634134 
[15]	train-rmse:0.361202	test-rmse:0.637910 
[16]	train-rmse:0.347489	test-rmse:0.636140 
[17]	train-rmse:0.333721	test-rmse:0.640103 
[18]	train-rmse:0.327205	test-rmse:0.640305 
[19]	train-rmse:0.322311	test-rmse:0.640570 
[20]	train-rmse:0.311274	test-rmse:0.637183 
[21]	train-rmse:0.306126	test-rmse:0.639540 
[22]	train-rmse:0.304587	test-rmse:0.641046 
[23]	train-rmse:0.302227	test-rmse:0.642247 
[24]	train-rmse:0.291819	test-rmse:0.642033 
[25]	train-rmse:0.283164	test-rmse:0.643694 
[26]	train-rmse:0.274758	test-rmse:0.644476 
[27]	train-rmse:0.263478	test-rmse:0.644651 
[28]	train-rmse:0.261211	test-rmse:0.645701 
[29]	train-rmse:0.252939	test-rmse:0.645628 
[30]	train-rmse:0.242802	test-rmse:0.645754 
[31]	train-rmse:0.235155	test-rmse:0.645139 
[32]	train-rmse:0.223787	test-rmse:0.647024 
[33]	train-rmse:0.215584	test-rmse:0.646311 
[34]	train-rmse:0.210648	test-rmse:0.645720 
[35]	train-rmse:0.207097	test-rmse:0.646027 
[36]	train-rmse:0.205686	test-rmse:0.646187 
[37]	train-rmse:0.203362	test-rmse:0.647617 
[38]	train-rmse:0.195475	test-rmse:0.648483 
[39]	train-rmse:0.192458	test-rmse:0.649351 
[40]	train-rmse:0.188874	test-rmse:0.648917 
[41]	train-rmse:0.183996	test-rmse:0.647272 
[42]	train-rmse:0.178708	test-rmse:0.648906 
[43]	train-rmse:0.170608	test-rmse:0.650643 
[44]	train-rmse:0.165917	test-rmse:0.649631 
[45]	train-rmse:0.158432	test-rmse:0.648941 
[46]	train-rmse:0.152803	test-rmse:0.648363 
[47]	train-rmse:0.149589	test-rmse:0.648510 
[48]	train-rmse:0.147823	test-rmse:0.648336 
[49]	train-rmse:0.147145	test-rmse:0.648266 
[50]	train-rmse:0.141775	test-rmse:0.648939 
~~~
{: .output}


~~~
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
~~~
{: .language-r}

<img src="../fig/rmd-05-unnamed-chunk-7-1.png" title="plot of chunk unnamed-chunk-7" alt="plot of chunk unnamed-chunk-7" width="612" style="display: block; margin: auto;" />


## Overfitting

The RMSE on the training set is much smaller than the RMSE on the test set, so our model is *overfitting*. 


~~~
gbm <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 100,
               max_depth = 3,
               eta = 0.03,
               )
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
~~~
{: .language-r}

<img src="../fig/rmd-05-unnamed-chunk-8-1.png" title="plot of chunk unnamed-chunk-8" alt="plot of chunk unnamed-chunk-8" width="612" style="display: block; margin: auto;" />

~~~
tail(gbm$evaluation_log)
~~~
{: .language-r}



~~~
   iter train_rmse test_rmse
1:   95   0.636972  0.709072
2:   96   0.632454  0.705320
3:   97   0.628133  0.701641
4:   98   0.623801  0.698088
5:   99   0.619887  0.695051
6:  100   0.616225  0.692573
~~~
{: .output}

TODO: Challenge: Tune some parameters and try to get a model with less overfitting. Start with max_depth = 6 and eta = 0.3 (default) and tweak. Possible solution above.


TODO: Challenge: Try with white wine data.
