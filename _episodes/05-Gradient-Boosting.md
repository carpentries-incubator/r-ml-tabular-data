---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 05-Gradient-Boosting.md in _episodes_rmd/
source: Rmd
title: "Gradient Boosted Trees"
teaching: 50 
exercises: 15
questions:
- "What is gradient boosting?"
- "TODO"
objectives:
- "TODO"
keypoints:
- "TODO"
---



## Gradient Boosted Trees

TODO: 


## Reload the red wine data


~~~
library(tidyverse)
library(here)
~~~
{: .language-r}


~~~
library(xgboost)
~~~
{: .language-r}



~~~

Attaching package: 'xgboost'
~~~
{: .output}



~~~
The following object is masked from 'package:dplyr':

    slice
~~~
{: .output}

Notice that both `xgboost` and `dplyr` have a function called `slice`. In the following code block, we specify that we want to use the `dplyr` version.


~~~
library(tidyverse)
library(here)
wine <- read_csv(here("data", "wine.csv"))
redwine <- wine %>% dplyr::slice(1:1599) 
trainSize <- round(0.80 * nrow(redwine))
set.seed(1234) 
trainIndex <- sample(nrow(redwine), trainSize)
trainDF <- redwine %>% dplyr::slice(trainIndex)
testDF <- redwine %>% dplyr::slice(-trainIndex)
~~~
{: .language-r}

## Regression Model


~~~
gbm <- xgboost(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality, nrounds = 50)
~~~
{: .language-r}



~~~
[1]	train-rmse:3.676364 
[2]	train-rmse:2.617381 
[3]	train-rmse:1.884501 
[4]	train-rmse:1.380320 
[5]	train-rmse:1.040359 
[6]	train-rmse:0.812738 
[7]	train-rmse:0.661508 
[8]	train-rmse:0.563880 
[9]	train-rmse:0.499910 
[10]	train-rmse:0.455213 
[11]	train-rmse:0.427036 
[12]	train-rmse:0.399291 
[13]	train-rmse:0.388654 
[14]	train-rmse:0.368818 
[15]	train-rmse:0.361202 
[16]	train-rmse:0.347489 
[17]	train-rmse:0.333721 
[18]	train-rmse:0.327205 
[19]	train-rmse:0.322311 
[20]	train-rmse:0.311274 
[21]	train-rmse:0.306126 
[22]	train-rmse:0.304587 
[23]	train-rmse:0.302227 
[24]	train-rmse:0.291819 
[25]	train-rmse:0.283164 
[26]	train-rmse:0.274758 
[27]	train-rmse:0.263478 
[28]	train-rmse:0.261211 
[29]	train-rmse:0.252939 
[30]	train-rmse:0.242802 
[31]	train-rmse:0.235155 
[32]	train-rmse:0.223787 
[33]	train-rmse:0.215584 
[34]	train-rmse:0.210648 
[35]	train-rmse:0.207097 
[36]	train-rmse:0.205686 
[37]	train-rmse:0.203362 
[38]	train-rmse:0.195475 
[39]	train-rmse:0.192458 
[40]	train-rmse:0.188874 
[41]	train-rmse:0.183996 
[42]	train-rmse:0.178708 
[43]	train-rmse:0.170608 
[44]	train-rmse:0.165917 
[45]	train-rmse:0.158432 
[46]	train-rmse:0.152803 
[47]	train-rmse:0.149589 
[48]	train-rmse:0.147823 
[49]	train-rmse:0.147145 
[50]	train-rmse:0.141775 
~~~
{: .output}


~~~
pQuality <- predict(gbm, as.matrix(select(testDF, -quality)))
gbRMSE <- sqrt(mean((pQuality - testDF$quality)^2))
gbRMSE
~~~
{: .language-r}



~~~
[1] 0.6489389
~~~
{: .output}

## More Details on the Training Process


~~~
dtrain <- xgb.DMatrix(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality)
dtest <- xgb.DMatrix(data = as.matrix(select(testDF, -quality)), label = testDF$quality)
watch <- list(train = dtrain, test = dtest)
~~~
{: .language-r}


~~~
gbm <- xgb.train(data = dtrain, watchlist = watch, nrounds = 50)
~~~
{: .language-r}



~~~
[1]	train-rmse:3.676364	test-rmse:3.676491 
[2]	train-rmse:2.617381	test-rmse:2.620643 
[3]	train-rmse:1.884501	test-rmse:1.904736 
[4]	train-rmse:1.380320	test-rmse:1.414777 
[5]	train-rmse:1.040359	test-rmse:1.110905 
[6]	train-rmse:0.812738	test-rmse:0.913236 
[7]	train-rmse:0.661508	test-rmse:0.793578 
[8]	train-rmse:0.563880	test-rmse:0.729415 
[9]	train-rmse:0.499910	test-rmse:0.686777 
[10]	train-rmse:0.455213	test-rmse:0.665393 
[11]	train-rmse:0.427036	test-rmse:0.650133 
[12]	train-rmse:0.399291	test-rmse:0.642655 
[13]	train-rmse:0.388654	test-rmse:0.640231 
[14]	train-rmse:0.368818	test-rmse:0.634134 
[15]	train-rmse:0.361202	test-rmse:0.637910 
[16]	train-rmse:0.347489	test-rmse:0.636140 
[17]	train-rmse:0.333721	test-rmse:0.640103 
[18]	train-rmse:0.327205	test-rmse:0.640305 
[19]	train-rmse:0.322311	test-rmse:0.640570 
[20]	train-rmse:0.311274	test-rmse:0.637183 
[21]	train-rmse:0.306126	test-rmse:0.639540 
[22]	train-rmse:0.304587	test-rmse:0.641046 
[23]	train-rmse:0.302227	test-rmse:0.642247 
[24]	train-rmse:0.291819	test-rmse:0.642033 
[25]	train-rmse:0.283164	test-rmse:0.643694 
[26]	train-rmse:0.274758	test-rmse:0.644476 
[27]	train-rmse:0.263478	test-rmse:0.644651 
[28]	train-rmse:0.261211	test-rmse:0.645701 
[29]	train-rmse:0.252939	test-rmse:0.645628 
[30]	train-rmse:0.242802	test-rmse:0.645754 
[31]	train-rmse:0.235155	test-rmse:0.645139 
[32]	train-rmse:0.223787	test-rmse:0.647024 
[33]	train-rmse:0.215584	test-rmse:0.646311 
[34]	train-rmse:0.210648	test-rmse:0.645720 
[35]	train-rmse:0.207097	test-rmse:0.646027 
[36]	train-rmse:0.205686	test-rmse:0.646187 
[37]	train-rmse:0.203362	test-rmse:0.647617 
[38]	train-rmse:0.195475	test-rmse:0.648483 
[39]	train-rmse:0.192458	test-rmse:0.649351 
[40]	train-rmse:0.188874	test-rmse:0.648917 
[41]	train-rmse:0.183996	test-rmse:0.647272 
[42]	train-rmse:0.178708	test-rmse:0.648906 
[43]	train-rmse:0.170608	test-rmse:0.650643 
[44]	train-rmse:0.165917	test-rmse:0.649631 
[45]	train-rmse:0.158432	test-rmse:0.648941 
[46]	train-rmse:0.152803	test-rmse:0.648363 
[47]	train-rmse:0.149589	test-rmse:0.648510 
[48]	train-rmse:0.147823	test-rmse:0.648336 
[49]	train-rmse:0.147145	test-rmse:0.648266 
[50]	train-rmse:0.141775	test-rmse:0.648939 
~~~
{: .output}


~~~
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
~~~
{: .language-r}

<img src="../fig/rmd-05-unnamed-chunk-9-1.png" title="plot of chunk unnamed-chunk-9" alt="plot of chunk unnamed-chunk-9" width="612" style="display: block; margin: auto;" />


## Overfitting

The RMSE on the training set is much smaller than the RMSE on the test set, so our model is *overfitting*. 


~~~
gbm <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 140,
               max_depth = 3,
               eta = 0.03,
               )
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
~~~
{: .language-r}

<img src="../fig/rmd-05-unnamed-chunk-10-1.png" title="plot of chunk unnamed-chunk-10" alt="plot of chunk unnamed-chunk-10" width="612" style="display: block; margin: auto;" />

~~~
tail(gbm$evaluation_log)
~~~
{: .language-r}



~~~
   iter train_rmse test_rmse
1:  135   0.552674  0.650107
2:  136   0.552007  0.649781
3:  137   0.551449  0.649636
4:  138   0.550516  0.649335
5:  139   0.549945  0.649244
6:  140   0.549213  0.649080
~~~
{: .output}

TODO: Challenge: Tune some parameters and try to get a model with less overfitting. Start with max_depth = 6 and eta = 0.3 (default) and tweak max_depth, eta, and nrounds. Possible solution above.


TODO: Challenge: Try with white wine data.


~~~
whitewine <- wine %>% dplyr::slice(1600:6497) 
trainSize <- round(0.80 * nrow(whitewine))
set.seed(1234) 
trainIndex <- sample(nrow(whitewine), trainSize)
trainDF <- whitewine %>% dplyr::slice(trainIndex)
testDF <- whitewine %>% dplyr::slice(-trainIndex)
dtrain <- xgb.DMatrix(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality)
dtest <- xgb.DMatrix(data = as.matrix(select(testDF, -quality)), label = testDF$quality)
watch <- list(train = dtrain, test = dtest)
~~~
{: .language-r}


~~~
gbm <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,

               nrounds = 15)0,
               max_depth = 3,
               eta = 0.03,
               )
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
tail(gbm$evaluation_log)
~~~
{: .language-r}
