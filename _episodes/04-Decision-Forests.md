---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 04-Decision-Forests.md in _episodes_rmd/
source: Rmd
title: "Decision Forests"
teaching: 50 
exercises: 20
questions:
- "What are decision forests?"
- "How can we use a decision tree model to make a prediction?"
- "How do decision forests improve decision tree models?"
objectives:
- "Introduce decision forests."
- "Use decision forests for classification and regression models."
- "Evaluate the quality of a decision forest model."
keypoints:
- "Decision forests can make predictions of a categorical or quantitative variable."
- "Decision forests, with their default settings, work reasonably well."
---



## Decision Forests

TODO:

## Wine Dataset

For this episode, we will use a data set described in the [article](https://doi.org/10.1016/j.dss.2009.05.016) *Modeling wine preferences by data mining from physicochemical properties*, in Decision Support Systems, 47(4):547-553, by P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. The data set contains quality ratings and measurements from 6497 samples of wine; rows `1:1599` are red wine samples, and rows `1600:6497` are white wine. 


~~~
library(tidyverse)
library(here)
wine <- read_csv(here("data", "wine.csv"))
~~~
{: .language-r}


~~~
glimpse(wine)
~~~
{: .language-r}



~~~
Rows: 6,497
Columns: 12
$ fixed.acidity        <dbl> 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7.5…
$ volatile.acidity     <dbl> 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, …
$ citric.acid          <dbl> 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0…
$ residual.sugar       <dbl> 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.1,…
$ chlorides            <dbl> 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, …
$ free.sulfur.dioxide  <dbl> 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, 16…
$ total.sulfur.dioxide <dbl> 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 102,…
$ density              <dbl> 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0…
$ pH                   <dbl> 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3…
$ sulphates            <dbl> 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0…
$ alcohol              <dbl> 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 10.…
$ quality              <dbl> 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 7…
~~~
{: .output}



~~~
ggplot(wine, aes(x = quality)) + geom_histogram(binwidth = 1)
~~~
{: .language-r}

<img src="../fig/rmd-04-unnamed-chunk-3-1.png" title="plot of chunk unnamed-chunk-3" alt="plot of chunk unnamed-chunk-3" width="612" style="display: block; margin: auto;" />

## Red Wine Classification Model


~~~
redwineC <- wine %>%
  slice(1:1599) %>%
  mutate(grade = as_factor(if_else(quality < 5.5, "bad", "good"))) %>%  
  select(-quality)
summary(redwineC$grade)
~~~
{: .language-r}



~~~
 bad good 
 744  855 
~~~
{: .output}

## Create Training and Test Sets


~~~
trainSize <- round(0.80 * nrow(redwineC))
set.seed(1234) 
trainIndex <- sample(nrow(redwineC), trainSize)
trainDF <- redwineC %>% slice(trainIndex)
testDF <- redwineC %>% slice(-trainIndex)
~~~
{: .language-r}

## Fit a Decision Tree


~~~
library(rpart)
library(rpart.plot)
rwtree <- rpart(grade ~ ., data = trainDF, method = "class")
rpart.plot(rwtree)
~~~
{: .language-r}

<img src="../fig/rmd-04-unnamed-chunk-6-1.png" title="plot of chunk unnamed-chunk-6" alt="plot of chunk unnamed-chunk-6" width="612" style="display: block; margin: auto;" />



~~~
rwp <- predict(rwtree, testDF)
rwpred <- apply(rwp, 1, function(r) {names(which.max(r))})
sum(testDF$grade == rwpred)/nrow(testDF)
~~~
{: .language-r}



~~~
[1] 0.696875
~~~
{: .output}

## Now do it with a random forest



~~~
library(randomForest)
~~~
{: .language-r}



~~~
randomForest 4.7-1
~~~
{: .output}



~~~
Type rfNews() to see new features/changes/bug fixes.
~~~
{: .output}



~~~

Attaching package: 'randomForest'
~~~
{: .output}



~~~
The following object is masked from 'package:dplyr':

    combine
~~~
{: .output}



~~~
The following object is masked from 'package:ggplot2':

    margin
~~~
{: .output}



~~~
set.seed(4567)
rwfor <- randomForest(grade ~ ., data = trainDF)
rwpred2 <- predict(rwfor, testDF)
sum(testDF$grade == rwpred2)/nrow(testDF)
~~~
{: .language-r}



~~~
[1] 0.821875
~~~
{: .output}


## Examine our Random Forest


~~~
print(rwfor)
~~~
{: .language-r}



~~~

Call:
 randomForest(formula = grade ~ ., data = trainDF) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 19.16%
Confusion matrix:
     bad good class.error
bad  470  119   0.2020374
good 126  564   0.1826087
~~~
{: .output}

## Train on the whole data set


~~~
set.seed(567)
rwforFull <- randomForest(grade ~ ., data = redwineC)
print(rwforFull)
~~~
{: .language-r}



~~~

Call:
 randomForest(formula = grade ~ ., data = redwineC) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 16.64%
Confusion matrix:
     bad good class.error
bad  618  126   0.1693548
good 140  715   0.1637427
~~~
{: .output}

## Variable Importance


~~~
importance(rwforFull)
~~~
{: .language-r}



~~~
                     MeanDecreaseGini
fixed.acidity                53.36714
volatile.acidity             85.98563
citric.acid                  52.65787
residual.sugar               44.08492
chlorides                    59.12602
free.sulfur.dioxide          47.00822
total.sulfur.dioxide         80.23087
density                      71.76048
pH                           53.40917
sulphates                   104.83733
alcohol                     142.50949
~~~
{: .output}



~~~
importance(rwforFull) %>% 
  as_tibble(rownames = "Variable") %>% 
  arrange(desc(MeanDecreaseGini))
~~~
{: .language-r}



~~~
# A tibble: 11 × 2
   Variable             MeanDecreaseGini
   <chr>                           <dbl>
 1 alcohol                         143. 
 2 sulphates                       105. 
 3 volatile.acidity                 86.0
 4 total.sulfur.dioxide             80.2
 5 density                          71.8
 6 chlorides                        59.1
 7 pH                               53.4
 8 fixed.acidity                    53.4
 9 citric.acid                      52.7
10 free.sulfur.dioxide              47.0
11 residual.sugar                   44.1
~~~
{: .output}

## Red Wine Regression Model


~~~
redwineR <- wine %>% slice(1:1599) 
trainSize <- round(0.80 * nrow(redwineR))
set.seed(1234) 
trainIndex <- sample(nrow(redwineR), trainSize)
trainDF <- redwineR %>% slice(trainIndex)
testDF <- redwineR %>% slice(-trainIndex)
~~~
{: .language-r}

## Fit a Decision Tree

When the dependent variable is quantitative, we use the `anova` method to construct a decision tree.


~~~
rwtree <- rpart(quality ~ ., data = trainDF, method = "anova")
rpart.plot(rwtree)
~~~
{: .language-r}

<img src="../fig/rmd-04-unnamed-chunk-13-1.png" title="plot of chunk unnamed-chunk-13" alt="plot of chunk unnamed-chunk-13" width="612" style="display: block; margin: auto;" />

## Decision Tree RMSE


~~~
predictedQuality <- predict(rwtree, testDF)
errors <- predictedQuality - testDF$quality
dtRMSE <- sqrt(mean(errors^2))
dtRMSE
~~~
{: .language-r}



~~~
[1] 0.6862169
~~~
{: .output}

## Random Forest Regression Model



~~~
set.seed(4567)
rwfor <- randomForest(quality ~ ., data = trainDF)
print(rwfor)
~~~
{: .language-r}



~~~

Call:
 randomForest(formula = quality ~ ., data = trainDF) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 3

          Mean of squared residuals: 0.3269279
                    % Var explained: 49.5
~~~
{: .output}

The `% Var explained` term is a "pseudo R-squared", computed as $1 - \text{MSE}/\text{Var}(y)$.
The mean of squared residuals is based on the errors for the entire training set. Note that it's square root is close to the RMSE we calculated on the test set. 


~~~
rfRMSE <- sqrt(mean((predict(rwfor, testDF) - testDF$quality)^2))
rfRMSE
~~~
{: .language-r}



~~~
[1] 0.5913485
~~~
{: .output}



~~~
rfRMSE^2
~~~
{: .language-r}



~~~
[1] 0.3496931
~~~
{: .output}

You can also view the out-of-bag errors. The average OOB MSE is close to the MSE on the training set. So again, you don't really need a train-test split when working with decision forests.


~~~
mean(rwfor$mse)
~~~
{: .language-r}



~~~
[1] 0.3383084
~~~
{: .output}



~~~
importance(rwfor)
~~~
{: .language-r}



~~~
                     IncNodePurity
fixed.acidity             47.95386
volatile.acidity         103.36878
citric.acid               52.83630
residual.sugar            42.08948
chlorides                 52.91782
free.sulfur.dioxide       39.40336
total.sulfur.dioxide      64.97532
density                   70.79843
pH                        44.20004
sulphates                107.31104
alcohol                  157.09548
~~~
{: .output}



~~~
importance(rwfor) %>% 
  as_tibble(rownames = "Variable") %>% 
  arrange(desc(IncNodePurity))
~~~
{: .language-r}



~~~
# A tibble: 11 × 2
   Variable             IncNodePurity
   <chr>                        <dbl>
 1 alcohol                      157. 
 2 sulphates                    107. 
 3 volatile.acidity             103. 
 4 density                       70.8
 5 total.sulfur.dioxide          65.0
 6 chlorides                     52.9
 7 citric.acid                   52.8
 8 fixed.acidity                 48.0
 9 pH                            44.2
10 residual.sugar                42.1
11 free.sulfur.dioxide           39.4
~~~
{: .output}

## Linear Regression Model (Optional)


~~~
redwine.lm <- lm(quality ~ ., data = trainDF)
summary(redwine.lm)
~~~
{: .language-r}



~~~

Call:
lm(formula = quality ~ ., data = trainDF)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.68737 -0.36035 -0.03507  0.43456  1.95898 

Coefficients:
                       Estimate Std. Error t value Pr(>|t|)    
(Intercept)           9.3242378 23.4596015   0.397   0.6911    
fixed.acidity         0.0108114  0.0284758   0.380   0.7043    
volatile.acidity     -1.2144573  0.1320387  -9.198  < 2e-16 ***
citric.acid          -0.2957551  0.1608745  -1.838   0.0662 .  
residual.sugar        0.0193480  0.0168430   1.149   0.2509    
chlorides            -1.8858347  0.4583915  -4.114 4.14e-05 ***
free.sulfur.dioxide   0.0054883  0.0023783   2.308   0.0212 *  
total.sulfur.dioxide -0.0034664  0.0007974  -4.347 1.49e-05 ***
density              -5.0636470 23.9244350  -0.212   0.8324    
pH                   -0.4331191  0.2065623  -2.097   0.0362 *  
sulphates             0.9244109  0.1306583   7.075 2.47e-12 ***
alcohol               0.2886439  0.0293633   9.830  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.6385 on 1267 degrees of freedom
Multiple R-squared:  0.3762,	Adjusted R-squared:  0.3708 
F-statistic: 69.46 on 11 and 1267 DF,  p-value: < 2.2e-16
~~~
{: .output}



~~~
lmRMSE <- sqrt(mean((predict(redwine.lm, testDF) - testDF$quality)^2))
lmRMSE
~~~
{: .language-r}



~~~
[1] 0.6880957
~~~
{: .output}

Challenge: Train a random forest on entire `redwineR` dataset. Do the MSE and pseudo R-squared improve?

Solution:


~~~
set.seed(4567)
rwfor <- randomForest(quality ~ ., data = redwineR)
print(rwfor)
~~~
{: .language-r}


Challenge? White wine decision forest regression model (whole dataset). Are the important variables different for ratings of white wine?

Solution:


~~~
whitewineR <- wine %>% slice(1600:6497) 
set.seed(4567)
wwfor <- randomForest(quality ~ ., data = whitewineR)
print(wwfor)
importance(wwfor) %>% 
  as_tibble(rownames = "Variable") %>% 
  arrange(desc(IncNodePurity))
~~~
{: .language-r}

Note: We have correlated variables in this data set. Random forests handle them fairly well.

Challenge: Try increasing `mtry` and `ntree`. Do the results improve?

Solution:


~~~
set.seed(4567)
wwfor <- randomForest(quality ~ ., data = whitewineR, mtry = 5)
print(wwfor)
~~~
{: .language-r}


~~~
set.seed(4567)
wwfor <- randomForest(quality ~ ., data = whitewineR, ntree = 1000)
print(wwfor)
~~~
{: .language-r}

Neither of the above changes has much of an effect.
