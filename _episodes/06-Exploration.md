---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 06-Exploration.md in _episodes_rmd/
source: Rmd
title: "Cross Validation and Tuning"
teaching: 15 
exercises: 50
questions:
- "TODO"
objectives:
- "TODO"
keypoints:
- "TODO"
---



## Summary

TODO: 



# Hyperparameters


# Cross Validation


# Red Wine Data, revisited

## Reload Red Wine Training/Test set


~~~
library(tidyverse)
library(here)
library(xgboost)
wine <- read_csv(here("data", "wine.csv"))
redwine <- wine %>% dplyr::slice(1:1599) 
trainSize <- round(0.80 * nrow(redwine))
set.seed(1234) 
trainIndex <- sample(nrow(redwine), trainSize)
trainDF <- redwine %>% dplyr::slice(trainIndex)
testDF <- redwine %>% dplyr::slice(-trainIndex)
dtrain <- xgb.DMatrix(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality)
dtest <- xgb.DMatrix(data = as.matrix(select(testDF, -quality)), label = testDF$quality)
watch <- list(train = dtrain, test = dtest)
~~~
{: .language-r}


~~~
set.seed(8833)
rwCV <- xgb.cv(params = list(eta = 0.3),
               data = dtrain, 
               nfold = 10,
               nrounds = 500,
               early_stopping_rounds = 10,
               verbose = FALSE)
~~~
{: .language-r}

Examine cv results.


~~~
paramDF <- tibble(eta = c(0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4))
paramList <- lapply(split(paramDF, 1:nrow(paramDF)), as.list)
bestResults <- tibble()
set.seed(8833)
pb <- txtProgressBar(style = 3)
for(i in seq(length(paramList))) {
  rwCV <- xgb.cv(params = paramList[[i]], 
                 data = dtrain, 
                 nrounds = 500, 
                 nfold = 10,
                 early_stopping_rounds = 10,
                 verbose = FALSE)
  bestResults <- bestResults %>% 
    bind_rows(rwCV$evaluation_log[rwCV$best_iteration])
  gc() # Free unused memory after each loop iteration
  setTxtProgressBar(pb, i/length(paramList))
}
close(pb)
etasearch <- bind_cols(paramDF, bestResults)
~~~
{: .language-r}

Use eta = 0.1 for parameter tuning. 

## Grid search


~~~
paramDF <- expand.grid(
  max_depth = 5:13,
  min_child_weight = seq(0, 20, by = 2),
  eta = 0.1
)
paramList <- lapply(split(paramDF, 1:nrow(paramDF)), as.list)
bestResults <- tibble()
set.seed(8833)
pb <- txtProgressBar(style = 3)
for(i in seq(length(paramList))) {
  rwCV <- xgb.cv(params = paramList[[i]],
                 data = dtrain, 
                 nrounds = 500, 
                 nfold = 10,
                 early_stopping_rounds = 10,
                 verbose = FALSE)
  bestResults <- bestResults %>% 
    bind_rows(rwCV$evaluation_log[rwCV$best_iteration])
  gc() # Free unused memory after each loop iteration
  setTxtProgressBar(pb, i/length(paramList))
}
close(pb)
depth_childweight <- bind_cols(paramDF, bestResults)
~~~
{: .language-r}

Looks like deep trees do better. 


~~~
paramDF <- expand.grid(
  max_depth = seq(10, 30, by = 2),
  max_leaves = c(15, 31, 63, 127, 255, 511, 1023, 2047, 4095),
  eta = 0.1
)
paramList <- lapply(split(paramDF, 1:nrow(paramDF)), as.list)
bestResults <- tibble()
set.seed(8833)
pb <- txtProgressBar(style = 3)
for(i in seq(length(paramList))) {
  rwCV <- xgb.cv(params = paramList[[i]],
                 data = dtrain, 
                 nrounds = 500, 
                 nfold = 10,
                 early_stopping_rounds = 10,
                 verbose = FALSE)
  bestResults <- bestResults %>% 
    bind_rows(rwCV$evaluation_log[rwCV$best_iteration])
  gc() # Free unused memory after each loop iteration
  setTxtProgressBar(pb, i/length(paramList))
}
close(pb)
depth_leaves <- bind_cols(paramDF, bestResults)
~~~
{: .language-r}

## Challenge: Do a grid search for subsample and colsample_bytree

Solution:


~~~
paramDF <- expand.grid(
  subsample = seq(0.5, 1, by = 0.1),
  colsample_bytree = seq(0.5, 1, by = 0.1),
  max_depth = 24,
  max_leaves = 2047,
  eta = 0.1
)
paramList <- lapply(split(paramDF, 1:nrow(paramDF)), as.list)
bestResults <- tibble()
set.seed(8833)
pb <- txtProgressBar(style = 3)
for(i in seq(length(paramList))) {
  rwCV <- xgb.cv(params = paramList[[i]],
                 data = dtrain, 
                 nrounds = 500, 
                 nfold = 10,
                 early_stopping_rounds = 10,
                 verbose = FALSE)
  bestResults <- bestResults %>% 
    bind_rows(rwCV$evaluation_log[rwCV$best_iteration])
  gc() 
  setTxtProgressBar(pb, i/length(paramList))
}
close(pb)
randsubsets <- bind_cols(paramDF, bestResults)
~~~
{: .language-r}

## Final check against test set


~~~
rwMod <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 10000,
               early_stopping_rounds = 50,
               max_depth = 24,
               max_leaves = 2047,
               subsample = 0.7,
               colsample_bytree = 0.5,
               eta = 0.01
               )
rwMod$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
print(rwMod)
~~~
{: .language-r}


## CHALLENGE: Further tuning on white wine data


~~~
source("loadwinedata.R")
paramGrid <- tibble(eta = seq(0.001, 0.4, length.out = 50))
paramList <- lapply(split(paramGrid, 1:nrow(paramGrid)), as.list)
bestResults <- tibble()
for(i in seq(length(paramList))) {
  toxCV <- xgb.cv(params = c(paramList[[i]], 
                             objective = "reg:squarederror",
                             tree_method = "hist"), 
                  data = dtrain, 
                  nrounds = 500, 
                  nfold = 10,
                  early_stopping_rounds = 10,
                  verbose = TRUE,
                  print_every_n = 10)
  gc()
  bestResults <- rbind(bestResults, toxCV$evaluation_log[toxCV$best_iteration])
}
etasearch <- bind_cols(paramGrid, bestResults)
~~~
{: .language-r}

All values of eta are overfitting. Let's go with 0.09 for tuning experiments. See if we can beat a test RMSE of 0.6135.


~~~
source("loadwinedata.R")
paramGrid <- expand.grid(
  max_depth = 5:13,
  min_child_weight = seq(0, 20, by = 2)
)
paramList <- lapply(split(paramGrid, 1:nrow(paramGrid)), as.list)
bestResults <- tibble()
print(Sys.time())
for(i in seq(length(paramList))) {
  cat("   #", i, "of", length(paramList))
  gbmod <- xgb.cv(params = c(paramList[[i]], 
                             eta = 0.1, # for parameter tuning
                             objective = "reg:squarederror",
                           #  tree_method = "hist"), 
                             tree_method = "exact"), 
                  data = dtrain, 
                  nrounds = 500, 
                  nfold = 10,
                  early_stopping_rounds = 10,
                  verbose = FALSE,
                  print_every_n = 10)
  bestResults <- rbind(bestResults, gbmod$evaluation_log[gbmod$best_iteration])
  gc() # Free unused memory after each loop iteration
}
cat("Finished.\n")
print(Sys.time())
depth_childweight <- bind_cols(paramGrid, bestResults)
~~~
{: .language-r}

Looks like max_depth = 3 and min_child_weight = 50 doesn't overfit too bad. Try it:

However max_depth = 12 and min_child_weight = 8 gives a test rmse of 0.6058.



~~~
gbmod <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 1000,
               early_stopping_rounds = 25,
               max_depth = 12,
               min_child_weight = 8,
               eta = 0.05
               )
gbmod$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
tail(gbmod$evaluation_log)
~~~
{: .language-r}
Best so far: test_rmse = 0.629. 



~~~
pww <- predict(gbmod, as.matrix(select(testDF, -quality)))
gbErrors <- pww - testDF$quality
tibble(`Predicted Quality` = pww, Error = gbErrors) %>%
  ggplot(aes(x = `Predicted Quality`, y = Error))  +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 0, intercept = 0) +
  theme_bw()
~~~
{: .language-r}



~~~
xgb.importance(model = gbmod)
~~~
{: .language-r}

## Tune using max_depth only?


~~~
source("loadwinedata.R")
paramGrid <- expand.grid(
  max_depth = 1:20
)
paramList <- lapply(split(paramGrid, 1:nrow(paramGrid)), as.list)
bestResults <- tibble()
for(i in seq(length(paramList))) {
  cat("   #", i, "of", length(paramList))
  gbmod <- xgb.cv(params = c(paramList[[i]], 
                             eta = 0.1, # for parameter tuning
                             objective = "reg:squarederror",
                             tree_method = "hist"), 
                  data = dtrain, 
                  nrounds = 500, 
                  nfold = 10,
                  early_stopping_rounds = 10,
                  verbose = FALSE,
                  print_every_n = 10)
  gc()
  bestResults <- rbind(bestResults, gbmod$evaluation_log[gbmod$best_iteration])
}
cat("Finished.\n")
depth_only <- bind_cols(paramGrid, bestResults)
~~~
{: .language-r}

# Lauree++ recommended method

https://sites.google.com/view/lauraepp/parameters

## Tune using max_depth and max_leaves


~~~
source("loadwinedata.R")
paramGrid <- expand.grid(
  max_depth = seq(10, 30, by = 2),
  max_leaves = c(15, 31, 63, 127, 255, 511, 1023, 2047, 4095)
)
paramList <- lapply(split(paramGrid, 1:nrow(paramGrid)), as.list)
bestResults <- tibble()
for(i in seq(length(paramList))) {
  cat("   #", i, "of", length(paramList))
  gbmod <- xgb.cv(params = c(paramList[[i]], 
                             eta = 0.1, # for parameter tuning
                             objective = "reg:squarederror",
                             tree_method = "hist"), 
                  data = dtrain, 
                  nrounds = 500, 
                  nfold = 10,
                  early_stopping_rounds = 10,
                  verbose = FALSE,
                  print_every_n = 10)
  gc()
  bestResults <- rbind(bestResults, gbmod$evaluation_log[gbmod$best_iteration])
}
cat("Finished.\n")
depth_leaves <- bind_cols(paramGrid, bestResults)
~~~
{: .language-r}

Best test-rmse is 12,63, followed by 26,255, 28,255.

## Grid for random sampling.


~~~
source("loadwinedata.R")
paramGrid <- expand.grid(
  subsample = seq(0.5, 1, by = 0.1),
  colsample_bytree = seq(0.5, 1, by = 0.1)
)
paramList <- lapply(split(paramGrid, 1:nrow(paramGrid)), as.list)
bestResults <- tibble()
set.seed(2341)
for(i in seq(length(paramList))) {
  cat("   #", i, "of", length(paramList))
  gbmod <- xgb.cv(params = c(paramList[[i]], 
                             max_depth = 28, 
                             max_leaves = 255,
                             eta = 0.1, # for parameter tuning
                             objective = "reg:squarederror",
                             tree_method = "hist"), 
                  data = dtrain, 
                  nrounds = 500, 
                  nfold = 10,
                  early_stopping_rounds = 10,
                  verbose = FALSE,
                  print_every_n = 10)
  gc()
  bestResults <- rbind(bestResults, gbmod$evaluation_log[gbmod$best_iteration])
}
cat("Finished.\n")
randrowcol <- bind_cols(paramGrid, bestResults)
~~~
{: .language-r}

0.602 (.9,.6)
.603  (.8, .9)

## Try tuning gamma?


~~~
source("loadwinedata.R")
paramGrid <- expand.grid(
  gamma = seq(0, 0.1, by = 0.01)
)
paramList <- lapply(split(paramGrid, 1:nrow(paramGrid)), as.list)
bestResults <- tibble()
set.seed(2341)
for(i in seq(length(paramList))) {
  cat("   #", i, "of", length(paramList))
  gbmod <- xgb.cv(params = c(paramList[[i]], 
                             max_depth = 28, 
                             max_leaves = 255,
                             subsample = 0.9,
                             colsample_bytree = 0.6,
                             eta = 0.1, # for parameter tuning
                             objective = "reg:squarederror",
                             tree_method = "hist"), 
                  data = dtrain, 
                  nrounds = 500, 
                  nfold = 10,
                  early_stopping_rounds = 10,
                  verbose = FALSE,
                  print_every_n = 10)
  gc()
  bestResults <- rbind(bestResults, gbmod$evaluation_log[gbmod$best_iteration])
}
cat("Finished.\n")
gammatest <- bind_cols(paramGrid, bestResults)
~~~
{: .language-r}

gamma = 0.06 gives a little bit of improvement.

## Final check against test set


~~~
gbmod <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 10000,
               early_stopping_rounds = 50,
               max_depth = 28,
               max_leaves = 255,
               subsample = 0.9,
               colsample_bytree = 0.6,
               gamma = 0.06,
               eta = 0.001
               )
gbmod$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
tail(gbmod$evaluation_log)
~~~
{: .language-r}
Best so far: test_rmse = 0.614885. 



~~~
pww <- predict(gbmod, as.matrix(select(testDF, -quality)))
gbErrors <- pww - testDF$quality
tibble(`Predicted Quality` = pww, Error = gbErrors) %>%
  ggplot(aes(x = `Predicted Quality`, y = Error))  +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 0, intercept = 0) +
  theme_bw()
~~~
{: .language-r}




## Challenge: Can you beat this tutorial?

## Facebook data: overfitting ok?

Data from UCI: https://archive.ics.uci.edu/ml/machine-learning-databases/00363/

Cambridge Spark tutorial: https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f


~~~
library(tidyverse)
library(here)
facebook <- read_csv(here("data", "facebook_comments", "Features_Variant_1.csv"), 
                     col_names = FALSE)
trainSize <- round(0.90 * nrow(facebook))
set.seed(123) 
trainIndex <- sample(nrow(facebook), trainSize)
trainDF <- facebook %>% dplyr::slice(trainIndex)
testDF <- facebook %>% dplyr::slice(-trainIndex)
  
library(xgboost)
dtrain <- xgb.DMatrix(data = as.matrix(select(trainDF, -X54)), label = trainDF$X54)
dtest <- xgb.DMatrix(data = as.matrix(select(testDF, -X54)), label = testDF$X54)
watch <- list(train = dtrain, test = dtest)

mean(abs(testDF$X54 - mean(trainDF$X54))) # should be 11.31?
~~~
{: .language-r}


~~~
fmod <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 1000,
               early_stopping_rounds = 10,
               max_depth = 6,
               min_child_weight = 1,
               eta = 0.3,
               subsample = 1,
               colsample_bytree = 1,
               objective = "reg:squarederror", # reg:linear is deprecated
               eval_metric = "mae"
               )
print(fmod) # MAE is a little different (4.31 in tutorial)
~~~
{: .language-r}


~~~
fmod$evaluation_log %>% 
  pivot_longer(cols = c(train_mae, test_mae), names_to = "MAE") %>% 
  ggplot(aes(x = iter, y = value, color = MAE)) + geom_line()
tail(fmod$evaluation_log)
~~~
{: .language-r}
Indeed there is quite a lot of overfitting.


~~~
predX54 <- predict(fmod, as.matrix(select(testDF, -X54)))
gbErrors <- predX54 - testDF$X54
tibble(Predicted = predX54, Error = gbErrors) %>%
  ggplot(aes(x = Predicted, y = Error))  +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 0, intercept = 0) +
  theme_bw()
~~~
{: .language-r}

Here's the "tuned" model:


~~~
fmod <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 1000,
               early_stopping_rounds = 10,
               max_depth = 10,
               min_child_weight = 6,
               eta = 0.01,
               subsample = 0.8,
               colsample_bytree = 1,
               objective = "reg:squarederror", # reg:linear is deprecated
               eval_metric = "mae"
               )
print(fmod) # MAE is a little different (3.90 in tutorial)
~~~
{: .language-r}


~~~
fmod$evaluation_log %>% 
  pivot_longer(cols = c(train_mae, test_mae), names_to = "MAE") %>% 
  ggplot(aes(x = iter, y = value, color = MAE)) + geom_line()
tail(fmod$evaluation_log)
~~~
{: .language-r}

Overfitting has improved somewhat, but it's still there. 
