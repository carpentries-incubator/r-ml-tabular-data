---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-Linear-Logistic.md in _episodes_rmd/
source: Rmd
title: "Linear and Logistic Regression"
math: true
teaching: 40
exercises: 10
questions:
- "How can a model make predictions?"
- "How do we judge the accuracy of predictions?"
objectives:
- "Define a linear regression model."
- "Define a logistic regression model."
- "Split data into training and testing sets."
keypoints:
- "Regression models can make predictions."
- "Testing sets can be used to measure the accuracy of a model."
---



## Kyphosis Data

Make sure the `rpart` package is loaded, and examine the structure of the `kyphosis` data frame.


~~~
library(rpart)
str(kyphosis)
~~~
{: .language-r}



~~~
'data.frame':	81 obs. of  4 variables:
 $ Kyphosis: Factor w/ 2 levels "absent","present": 1 1 2 1 1 1 1 1 1 2 ...
 $ Age     : int  71 158 128 2 1 1 61 37 113 59 ...
 $ Number  : int  3 3 4 5 4 2 2 3 2 6 ...
 $ Start   : int  5 14 5 1 15 16 17 16 16 12 ...
~~~
{: .output}

Notice that there are 81 observations of four variables, so this is a rather small data set for machine learning techniques. In this episode, we will use this data set to illustrate the process of training and testing, where our models will be built using classical linear and logistic regression. 

## Make a training set and a test set

The first step in the process is to create a random train/test split of our data set. There are various R packages that automate such tasks, but it is illustrative to use base R for now.

The following commands will randomly select the row indexes of the training set (and therefore also of the testing set).


~~~
trainSize <- round(0.75 * nrow(kyphosis))
set.seed(6789) # so we all get the same random sets
trainIndex <- sample(nrow(kyphosis), trainSize)
~~~
{: .language-r}

Take a look at the `trainIndex` variable in the Environment tab of RStudio. Since we set a particular value for the random seed, we should all see the same sample of random numbers.

Next we form two data frames using these indexes. Recall that the selection of `-trainIndex` will select all rows whose indexes are *not* in the `trainIndex` vector.


~~~
trainDF <- kyphosis[trainIndex, ]
testDF <- kyphosis[-trainIndex, ]
~~~
{: .language-r}

We can `View` the train and test sets in RStudio to check that they form a random partition of the `kyphosis` data.


~~~
View(trainDF)
View(testDF)
~~~
{: .language-r}

## Linear Regression as Supervised Learning

In the previous episode, we constructed a scatterplot of `Number` versus `Start` and observed a slight negative assocition. We can model this association with a linear function 

$$ 
\text{Start} = a + b \cdot \text{Number}
$$
where $a$ and $b$ are the intercept and slope, respectively, of the least squares regression line. To compute $a$ and $b$, we use the `lm` function in R.


~~~
model1 <- lm(Start ~ Number, data = trainDF)
summary(model1)
~~~
{: .language-r}



~~~

Call:
lm(formula = Start ~ Number, data = trainDF)

Residuals:
    Min      1Q  Median      3Q     Max 
-11.018  -1.610   1.615   3.186   5.798 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  16.4268     1.6393  10.021 2.38e-14 ***
Number       -1.2041     0.3721  -3.236  0.00199 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.698 on 59 degrees of freedom
Multiple R-squared:  0.1507,	Adjusted R-squared:  0.1364 
F-statistic: 10.47 on 1 and 59 DF,  p-value: 0.001988
~~~
{: .output}

The predicted `Start` is obtained by multiplying `Number` by -1.2041 and adding 16.4268.

> ## Challenge: Make a prediction
>
> Predict the number of the topmost vertebra when the number of
> vertebrae involved is 3.
>
> > ## Solution
> > 
> > Three times -1.2041 plus 
> > 16.4268 
> > is approximately 
> > 12.81.
> > 
> {: .solution}
{: .challenge}

## Try the Testing Data Set

In R there is a generic method called `predict` that will make predictions given models of various types. For example, we can compute the predicted starting vertebrae for all the cases in our testing set as follows.


~~~
predictedStart <- predict(model1, testDF)
~~~
{: .language-r}

> ## Challenge: Check our prediction
>
> Check that the result of the `predict` function agrees with the 
> result of the previous challenge.
>
> > ## Solution
> > 
> > 
> > ~~~
> > head(predictedStart)
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> >       12       21       32       36       37       38 
> > 12.81438 14.01851 14.01851 12.81438 12.81438 10.40611 
> > ~~~
> > {: .output}
> > 
> > 
> > 
> > ~~~
> > head(testDF)
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> >    Kyphosis Age Number Start
> > 12   absent 148      3    16
> > 21   absent  22      2    16
> > 32   absent 125      2    11
> > 36   absent  93      3    16
> > 37   absent   1      3     9
> > 38  present  52      5     6
> > ~~~
> > {: .output}
> > 
> > Notice that the first row of our testing set has a `Number` value of 3,
> > and the first value of `predictedStart` agrees with our answer to the
> > previous challenge.
> > 
> {: .solution}
{: .challenge}

Notice that, in general, the value of `Start` predicted by the model will not equal the actual value of `Start` in the testing set. However, in an accurate model, we would hope that the predicted values will be close to the actual values. To assess how close our predictions are to reality, we compute a vector of errors: predicted values minus actual values.


~~~
actualStart <- testDF$Start
errors <- predictedStart - actualStart
cat(round(errors, 1))
~~~
{: .language-r}



~~~
-3.2 -2 3 -3.2 3.8 4.4 0.2 2.6 10.6 2.8 -3.4 0.4 -3 -0.2 -0.2 1.6 -1.4 0.6 -5.6 -3.4
~~~
{: .output}

## Measuring the Prediction Error

There are several ways to summarize the overall error in a regression model. The average error is not a good choice, because errors will usually have positive and negative values, which cancel. To avoid this cancellation effect, we can take the mean of the squares of the errors: the *Mean Squared Error*, or MSE.


~~~
mean(errors^2)
~~~
{: .language-r}



~~~
[1] 13.14172
~~~
{: .output}

An alternative that has the same units as the output is the square root of the MSE: the *Root Mean Squared Error*, or RMSE.


~~~
sqrt(mean(errors^2))
~~~
{: .language-r}



~~~
[1] 3.625151
~~~
{: .output}

> ## Challenge: Mean Absolute Error
>
> The *Mean Absolute Error* (MAE) is the average of the absolute values of the
> errors.  Compute the MAE for the above example.
>
> > ## Solution
> > 
> > 
> > ~~~
> > mean(abs(errors))
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > [1] 2.77752
> > ~~~
> > {: .output}
> > 
> {: .solution}
{: .challenge}

We will compare most of the regression models that follow using the RMSE of the prediction error on the testing set.

## Logistic Regression

TODO: Density plots

TODO: Logistic model


~~~
model2 <- glm(Kyphosis ~ Age + Number + Start, data = trainDF, family = "binomial")
~~~
{: .language-r}

Shorthand notation:


~~~
model2 <- glm(Kyphosis ~ ., data = trainDF, family = "binomial")
~~~
{: .language-r}


~~~
predict(model2, testDF, type = "response")
~~~
{: .language-r}



~~~
         12          21          32          36          37          38 
0.060991556 0.004837329 0.066676823 0.027771160 0.034862103 0.389677526 
         39          43          44          46          47          51 
0.287882930 0.988363673 0.531083119 0.183999574 0.122064478 0.244337950 
         52          57          60          66          68          69 
0.003170923 0.014410381 0.061131542 0.069057143 0.236889844 0.056456800 
         70          76 
0.035581184 0.206575349 
~~~
{: .output}


~~~
levels(kyphosis$Kyphosis)
~~~
{: .language-r}



~~~
[1] "absent"  "present"
~~~
{: .output}


~~~
predictedKyphosis <- ifelse(predict(model2, testDF, type = "response") < 0.5,
                            "absent", "present")
predictedKyphosis
~~~
{: .language-r}



~~~
       12        21        32        36        37        38        39        43 
 "absent"  "absent"  "absent"  "absent"  "absent"  "absent"  "absent" "present" 
       44        46        47        51        52        57        60        66 
"present"  "absent"  "absent"  "absent"  "absent"  "absent"  "absent"  "absent" 
       68        69        70        76 
 "absent"  "absent"  "absent"  "absent" 
~~~
{: .output}

TODO: accuracy measurement


~~~
testDF$Kyphosis == predictedKyphosis
~~~
{: .language-r}



~~~
   12    21    32    36    37    38    39    43    44    46    47    51    52 
 TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE  TRUE 
   57    60    66    68    69    70    76 
 TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE 
~~~
{: .output}


~~~
accuracy <- sum(testDF$Kyphosis == predictedKyphosis)/nrow(testDF)
cat("Proportion of correct predictions using testing data: ", accuracy, "\n")
~~~
{: .language-r}



~~~
Proportion of correct predictions using testing data:  0.8 
~~~
{: .output}

TODO: Challenge: Try a different random seed. Does the accuracy stay the same?

TODO: Challenge: Try using multinom from nnet package.
