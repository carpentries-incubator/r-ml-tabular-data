---
source: Rmd
title: "Gradient Boosted Trees"
teaching: 50 
exercises: 15
questions:
- "What is gradient boosting?"
- "TODO"
objectives:
- "TODO"
keypoints:
- "TODO"
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("05-")
```

## Gradient Boosted Trees

TODO: 


## Reload the red wine data

```{r, message=FALSE, results=FALSE, warning=FALSE}
library(tidyverse)
library(here)
```

```{r}
library(xgboost)
```

Notice that both `xgboost` and `dplyr` have a function called `slice`. In the following code block, we specify that we want to use the `dplyr` version.

```{r, message=FALSE, results=FALSE, warning=FALSE}
wine <- read_csv(here("data", "wine.csv"))
redwine <- wine %>% dplyr::slice(1:1599) 
trainSize <- round(0.80 * nrow(redwine))
set.seed(1234) 
trainIndex <- sample(nrow(redwine), trainSize)
trainDF <- redwine %>% dplyr::slice(trainIndex)
testDF <- redwine %>% dplyr::slice(-trainIndex)
```

## Gradient Boosted Regression Model

```{r}
gbm <- xgboost(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality, nrounds = 50)
```

```{r}
pQuality <- predict(gbm, as.matrix(select(testDF, -quality)))
gbErrors <- pQuality - testDF$quality
gbRMSE <- sqrt(mean(gbErrors^2))
gbRMSE
```

```{r}
tibble(`Predicted Quality` = pQuality, Error = gbErrors) %>%
  ggplot(aes(x = `Predicted Quality`, y = Error))  +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 0, intercept = 0) +
  theme_bw()
```

## More Details on the Training Process

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality)
dtest <- xgb.DMatrix(data = as.matrix(select(testDF, -quality)), label = testDF$quality)
watch <- list(train = dtrain, test = dtest)
```

```{r}
gbm <- xgb.train(data = dtrain, watchlist = watch, nrounds = 50)
```

```{r}
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
```


## Overfitting

The RMSE on the training set is much smaller than the RMSE on the test set, so our model is *overfitting*. 

```{r}
gbm <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 140,
               max_depth = 3,
               eta = 0.03,
               )
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
tail(gbm$evaluation_log)
```

TODO: Challenge: Tune some parameters and try to get a model with less overfitting. Start with max_depth = 6 and eta = 0.3 (default) and tweak max_depth, eta, and nrounds. Possible solution above.

```{r}
pQuality <- predict(gbm, as.matrix(select(testDF, -quality)))
gbErrors <- pQuality - testDF$quality
tibble(`Predicted Quality` = pQuality, Error = gbErrors) %>%
  ggplot(aes(x = `Predicted Quality`, y = Error))  +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 0, intercept = 0) +
  theme_bw()
```


TODO: Challenge: Try with white wine data.

```{r, eval = FALSE}
whitewine <- wine %>% dplyr::slice(1600:6497) 
trainSize <- round(0.80 * nrow(whitewine))
set.seed(1234) 
trainIndex <- sample(nrow(whitewine), trainSize)
trainDF <- whitewine %>% dplyr::slice(trainIndex)
testDF <- whitewine %>% dplyr::slice(-trainIndex)
dtrain <- xgb.DMatrix(data = as.matrix(select(trainDF, -quality)), label = trainDF$quality)
dtest <- xgb.DMatrix(data = as.matrix(select(testDF, -quality)), label = testDF$quality)
watch <- list(train = dtrain, test = dtest)
```

```{r, eval = FALSE}
gbm <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
               nrounds = 150,
               max_depth = 3,
               eta = 0.03,
               )
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
tail(gbm$evaluation_log)
```

Note: Using linear boosting doesn't improve things.

```{r, eval = FALSE}
gbm <- xgb.train(data = dtrain, watchlist = watch, verbose = 0,
                 booster = "gblinear", eta = 0.02,
               nrounds = 150
               )
gbm$evaluation_log %>% 
  pivot_longer(cols = c(train_rmse, test_rmse), names_to = "RMSE") %>% 
  ggplot(aes(x = iter, y = value, color = RMSE)) + geom_line()
tail(gbm$evaluation_log)
```