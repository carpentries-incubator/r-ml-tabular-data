---
source: Rmd
title: "Linear and Logistic Regression"
teaching: 50
exercises: 15
questions:
- "How can a model make predictions?"
- "How do we judge the accuracy of predictions?"
objectives:
- "Define a linear regression model."
- "Define a logistic regression model."
- "Split data into training and testing sets."
keypoints:
- "Regression models can make predictions."
- "Testing sets can be used to measure the accuracy of a model."
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("02-")
```

## Kyphosis Data

```{r}
library(rpart)
str(kyphosis)
```

For a description of this data set, you can view the help menu for `kyphosis`.

```{r eval=FALSE}
?kyphosis
```

```{r, message=FALSE, results=FALSE, warning = FALSE}
library(tidyverse)
ggplot(kyphosis, aes(x = Number, y = Start)) + geom_point()
```

> ## Challenge: Number and Start
>
> Do you notice a trend in the above scatterplot? In the context of
> the `kyphosis` data, why would there be such a trend?
>
> > ## Solution
> > 
> > There appears to be a weak, negative association between
> > `Number` and `Start`: larger values of `Number` correspond
> > to smaller values of `Start`. This correspondence makes sense,
> > because if more vertebrae are involved, the topmost vertebra would
> > have to be higher up. (The vertebrae are numbered starting from
> > the top.)
> > 
> {: .solution}
{: .challenge}


## Make a training set and a test set

```{r}
trainSize <- round(0.75 * nrow(kyphosis))
set.seed(6789) # so we all get the same random sets
trainIndex <- sample(nrow(kyphosis), trainSize)
trainDF <- kyphosis[trainIndex, ]
testDF <- kyphosis[-trainIndex, ]
```


## Linear Regression

```{r}
model1 <- lm(Start ~ Number, data = trainDF)
summary(model1)
```

The predicted `Start` is obtained by multiplying `Number` by `r round(model1$coefficients[2], 3)` and adding `r round(model1$coefficients[1], 3)`.

> ## Challenge: Make a prediction
>
> Predict the number of the topmost vertebra when the number of
> vertebrae involved is 6.
>
> > ## Solution
> > 
> > Six times `r round(model1$coefficients[2], 3)` plus 
> > `r round(model1$coefficients[1], 3)` 
> > is approximately 
> > `r round(6 * model1$coefficients[2] + model1$coefficients[1], 2)`.
> > 
> {: .solution}
{: .challenge}

## Try the Testing Data Set

How well will our model perform on data that it was not trained on?

```{r}
predictedStart <- predict(model1, testDF)
actualStart <- testDF$Start
errors <- predictedStart - actualStart
cat(round(errors, 1))
```

TODO: Explain RMSE

```{r}
sqrt(mean(errors^2))
```

> ## Challenge: Mean Absolute Error
>
> The *Mean Absolute Error* (MAE) is the average of the absolute values of the
> errors.  Compute the MAE for the above example.
>
> > ## Solution
> > 
> > ```{r}
> > mean(abs(errors))
> > ```
> > 
> {: .solution}
{: .challenge}

## Logistic Regression

TODO: Density plots

TODO: Logistic model

```{r}
model2 <- glm(Kyphosis ~ Age + Number + Start, data = trainDF, family = "binomial")
```

Shorthand notation:

```{r}
model2 <- glm(Kyphosis ~ ., data = trainDF, family = "binomial")
```

```{r}
predict(model2, testDF, type = "response")
```

```{r}
levels(kyphosis$Kyphosis)
```

```{r}
predictedKyphosis <- ifelse(predict(model2, testDF, type = "response") < 0.5,
                            "absent", "present")
predictedKyphosis
```

TODO: accuracy measurement

```{r}
testDF$Kyphosis == predictedKyphosis
```

```{r}
accuracy <- sum(testDF$Kyphosis == predictedKyphosis)/nrow(testDF)
cat("Proportion of correct predictions using testing data: ", accuracy, "\n")
```

TODO: Challenge: Try a different random seed. Does the accuracy stay the same?

TODO: Challenge: Try using multinom from nnet package.
