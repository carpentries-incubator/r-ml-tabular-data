---
source: Rmd
title: "Decision Trees"
teaching: 30 
exercises: 15
questions:
- "What are decision trees?"
- "How can we use a decision tree model to make a prediction?"
- "What are some shortcomings of decision tree models?"
objectives:
- "Introduce decision trees and recursive partitioning."
- "Revisit the Kyphosis example using a classification tree."
keypoints:
- "Training data can give us a decision tree model."
- "Decision trees are sensitive to changes in the training set."
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("03-")
```

## Decision Trees

Let's simulate a data set of exam scores, along with letter grades.

```{r, message=FALSE, results=FALSE, warning=FALSE}
library(tidyverse)
set.seed(456)
exam <- tibble(score = sample(80:100, 200, replace = TRUE)) %>%
  mutate(grade = as_factor(ifelse(score < 90, "B", "A")))
head(exam)
summary(exam)
```

Given only this data frame, can we recover the grading scale that was used to assign the letter grades? In other words, can we *partition* this data set into A's and B's, based on the exam score? The `rpart` command can form this partition for us, using the formula syntax we saw in the last episode.

```{r, fig.width = 8.5, fig.height = 4}
library(rpart)
library(rpart.plot)
examTree <- rpart(grade ~ score, data = exam)
rpart.plot(examTree)
```

The `rpart` function searches for the best way to split the data set into predicted values of the response variables, based on the explanatory variables. This [Introduction to Rpart](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf) has details on how the split is chosen. In this simple case, the `rpart` function was able to perfectly partition the data after only one split. We can tell `rpart.plot` to report the number of correctly-classified cases in each node by including the option `extra = 102`.

```{r, fig.width = 8.5, fig.height = 4}
rpart.plot(examTree, extra = 102)
```

Notice that in the top node, every case was classified as a B, so only the Bs were correctly classified. After the split, both child nodes were classified perfectly.

In more complex situations, the algorithm will continue to create further splits to improve its classification. This process is called *recursive* partitioning.

> ## Challenge: Use `rpart` on the `kyphosis` data
>
> Use the `rpart` function to create a decision tree using the `kyphosis` data
> set. As in the previous episode, the response variable is `Kyphosis`, and the
> explanatory varables are the remaining columns `Age`, `Number`, and `Start`. 
> Use `rpart.plot` to plot your tree model.
> How many of the 81 cases does this tree classify incorrectly?
>
> > ## Solution
> > 
> > ```{r}
> > ktree <- rpart(Kyphosis ~ ., data = kyphosis)
> > rpart.plot(ktree, extra = 102)
> > ```
> > 
> > In the two leftmost leaves, all of the cases are classified correctly. However,
> > in the three remaining leaves, there are 2, 3, and 8 incorrectly classified
> > cases, for a total of 13 misclassifications.
> > 
> {: .solution}
{: .challenge}

## Using Decision Trees for Supervised Learning

In order to compare the decision tree model to the logistic regression model in the previous episode, let's train the model on the training set and test in on the testing set. The following commands will form our training and testing set using the `slice` function, which is part of the tidyverse.

```{r}
trainSize <- round(0.75 * nrow(kyphosis))
set.seed(6789) # same seed as in the last episode
trainIndex <- sample(nrow(kyphosis), trainSize)
trainDF <- kyphosis %>% slice(trainIndex)
testDF <- kyphosis %>% slice(-trainIndex)
```

Now train the decision tree model on the training set

```{r, fig.width = 8.5, fig.height = 4}
treeModel <- rpart(Kyphosis ~ Age + Number + Start, data = trainDF)
rpart.plot(treeModel, extra = 102)
```

Notice that we obtain a simpler tree using the testing set instead of the full data set.

> ## Challenge: Training set accuracy
>
> What proportion of cases in the training set were classified correctly?
>
> > ## Solution
> > 
> > Reading the leaves from left to right, there were 34, 11, and 6 
> > correctly-classified cases:
> > 
> > ```{r}
> > (34+11+6)/nrow(trainDF)
> > ```
> > 
> {: .solution}
{: .challenge}

## Model predictions on the testing set

The `predict` function works on `rpart` models similarly to how it works on `lm` and `glm` models, but the output is a matrix of predicted probabilities.

```{r}
predict(treeModel, testDF)
```

To investigate the behavior of this model, bind the columns of the predicted probabilities to the testing set.

```{r}
predMatrix <- predict(treeModel, testDF)
testPredictions <- testDF %>% 
  bind_cols(predMatrix, .name_repair = "minimal")
```

> ## Challenge: Predicted probabilities
>
> Compare the results in the `testPredictions` data frame with the plot
> of `treeModel`. Can you explain how the model is calculating the 
> predicted probabilites?
>
> > ## Solution
> > 
> > Consider the first row of `testPredictions`. 
> > 
> > ```{r}
> > testPredictions[1,]
> > ```
> > 
> > Since the value of `Start` is greater than 13, we follow the "yes" branch
> > of the decision tree and land at the leftmost leaf, labeled "absent", with 
> > a probability of 34/36, which is approximately 0.9444. Similarly, consider row 8.
> > 
> > ```{r}
> > testPredictions[8,]
> > ```
> > 
> > Since the value of `Start` is less than 13, we follow the "no" branch. Then since
> > the value of `Number` is greater than 6, we follow the right branch to land on the 
> > leaf labeled "present", with a probability of 6/7, which is 0.8571.
> > 
> {: .solution}
{: .challenge}

## Testing set accuracy

To check the testing set accuracy, we can compare the first column of this matrix to 0.5 and assign the appropriate value of the `Kyphosis` variable.

```{r}
predictedKyphosis <- ifelse(predMatrix[,1] > 0.5, 
                            "absent", "present")
accuracy <- sum(testDF$Kyphosis == predictedKyphosis)/nrow(testDF)
cat("Proportion of correct predictions: ", accuracy, "\n")
```

In general, the accuracy on the testing set will be less than the accuracy on the training set.

> ## Challenge: Change the training set
>
> Repeat the construction of the decision tree model for the `kyphosis`
> data, but experiment with different values of the random seed to obtain
> different testing and training sets. Does the shape of the tree change?
> Does the testing set accuracy change?
>
> > ## Solution
> > 
> > ```{r, fig.width = 8.5, fig.height = 4}
> > set.seed(314159) # try a different seed
> > trainIndex <- sample(nrow(kyphosis), trainSize) # use the same size training set
> > trainDF <- kyphosis %>% slice(trainIndex)
> > testDF <- kyphosis %>% slice(-trainIndex)
> > treeModel <- rpart(Kyphosis ~ Age + Number + Start, data = trainDF)
> > rpart.plot(treeModel, extra = 102)
> > ```
> > 
> > Changing the seed for the train/test split resulted in a different 
> > decision tree.
> > 
> > ```{r}
> > predMatrix <- predict(treeModel, testDF)
> > predictedKyphosis <- ifelse(predMatrix[,1] > 0.5, "absent", "present")
> > accuracy <- sum(testDF$Kyphosis == predictedKyphosis)/nrow(testDF)
> > cat("Proportion of correct predictions: ", accuracy, "\n")
> > ```
> >
> > The testing set accuracy also changed.
> > 
> {: .solution}
{: .challenge}

## Robustness, or lack thereof

In this episode we have seen that the structure of the decision tree can vary quite a bit when we make small changes to the training data. Training the model on the whole data set resulted in a very different tree than what we obtained by training on a slightly smaller testing set. And changing the choice of testing set, even while maintaining its size, also altered the tree structure. When the structure of a model changes significantly for a small change in the training data, we say that the model is not *robust*. Non-robustness can be a problem, because one or two unusual observations can make a big difference in the conclusions we draw. In the next episode, we will explore *random forests*, which are more robust than individual decision tree models.


