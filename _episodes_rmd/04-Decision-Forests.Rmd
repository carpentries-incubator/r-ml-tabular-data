---
source: Rmd
title: "Decision Forests"
teaching: 50 
exercises: 20
questions:
- "What are decision forests?"
- "How can we use a decision tree model to make a prediction?"
- "How do decision forests improve decision tree models?"
objectives:
- "Introduce decision forests."
- "Use decision forests for classification and regression models."
- "Evaluate the quality of a decision forest model."
keypoints:
- "Decision forests can make predictions of a categorical or quantitative variable."
- "Decision forests, with their default settings, work reasonably well."
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("04-")
```

## Decision Forests

TODO:

## Wine Dataset

For this episode, we will use a data set described in the paper *Modeling wine preferences by data mining from physicochemical properties*, in Decision Support Systems, 47(4):547-553, by P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. The data set contains quality ratings and measurements from 6497 samples of wine; rows `1:1599` are red wine samples, and rows `1600:6497` are white wine. 

```{r}
library(tidyverse)
library(here)
wine <- read_csv(here("data", "wine.csv"))
str(wine)
ggplot(wine, aes(x = quality)) + geom_histogram(binwidth = 1)
```

## Red Wine Classification Model

```{r}
redwineC <- wine %>%
  slice(1:1599) %>%
  mutate(grade = as_factor(if_else(quality < 5.5, "bad", "good"))) %>%  
  select(-quality)
summary(redwineC$grade)
```

## Create Training and Test Sets

```{r}
trainSize <- round(0.80 * nrow(redwineC))
set.seed(1234) 
trainIndex <- sample(nrow(redwineC), trainSize)
trainDF <- redwineC %>% slice(trainIndex)
testDF <- redwineC %>% slice(-trainIndex)
```

## Fit a Decision Tree

```{r}
library(rpart)
library(rpart.plot)
rwtree <- rpart(grade ~ ., data = trainDF, method = "class")
rpart.plot(rwtree)
```


```{r}
rwp <- predict(rwtree, testDF)
rwpred <- apply(rwp, 1, function(r) {names(which.max(r))})
sum(testDF$grade == rwpred)/nrow(testDF)
```

## Now do it with a random forest


```{r}
library(randomForest)
set.seed(4567)
rwfor <- randomForest(grade ~ ., data = trainDF)
rwpred2 <- predict(rwfor, testDF)
sum(testDF$grade == rwpred2)/nrow(testDF)
```


## Examine our Random Forest

```{r}
print(rwfor)
```

## Train on the whole data set

```{r}
set.seed(567)
rwforFull <- randomForest(grade ~ ., data = redwineC)
print(rwforFull)
```

## Variable Importance

```{r}
importance(rwforFull)
importance(rwforFull) %>% 
  as_tibble(rownames = "Variable") %>% 
  arrange(desc(MeanDecreaseGini))
```

## Red Wine Regression Model

```{r}
redwineR <- wine %>% slice(1:1599) 
trainSize <- round(0.80 * nrow(redwineR))
set.seed(1234) 
trainIndex <- sample(nrow(redwineR), trainSize)
trainDF <- redwineR %>% slice(trainIndex)
testDF <- redwineR %>% slice(-trainIndex)
```

## Fit a Decision Tree

When the dependent variable is quantitative, we use the `anova` method to construct a decision tree.

```{r}
rwtree <- rpart(quality ~ ., data = trainDF, method = "anova")
rpart.plot(rwtree)
```

## Decision Tree RMSE

```{r}
predictedQuality <- predict(rwtree, testDF)
errors <- predictedQuality - testDF$quality
dtRMSE <- sqrt(mean(errors^2))
dtRMSE
```

## Random Forest Regression Model


```{r}
set.seed(4567)
rwfor <- randomForest(quality ~ ., data = trainDF)
print(rwfor)
```

The `% Var explained` term is a "pseudo R-squared", computed as $1 - \text{MSE}/\text{Var}(y)$.
The mean of squared residuals is based on the errors for the entire training set. Note that it's square root is close to the RMSE we calculated on the test set. 

```{r}
rfRMSE <- sqrt(mean((predict(rwfor, testDF) - testDF$quality)^2))
rfRMSE
rfRMSE^2
```

You can also view the out-of-bag errors. The average OOB MSE is close to the MSE on the training set. So again, you don't really need a train-test split when working with decision forests.

```{r}
mean(rwfor$mse)
```


```{r}
importance(rwfor)
importance(rwfor) %>% 
  as_tibble(rownames = "Variable") %>% 
  arrange(desc(IncNodePurity))
```

## Linear Regression Model (Optional)

```{r}
redwine.lm <- lm(quality ~ ., data = trainDF)
summary(redwine.lm)
lmRMSE <- sqrt(mean((predict(redwine.lm, testDF) - testDF$quality)^2))
lmRMSE
```

Challenge: Train a random forest on entire `redwineR` dataset. Do the MSE and pseudo R-squared improve?

Solution:

```{r, eval = FALSE}
set.seed(4567)
rwfor <- randomForest(quality ~ ., data = redwineR)
print(rwfor)
```


Challenge? White wine decision forest regression model (whole dataset). Are the important variables different for ratings of white wine?

Solution:

```{r, eval = FALSE}
whitewineR <- wine %>% slice(1600:6497) 
set.seed(4567)
wwfor <- randomForest(quality ~ ., data = whitewineR)
print(wwfor)
importance(wwfor) %>% 
  as_tibble(rownames = "Variable") %>% 
  arrange(desc(IncNodePurity))
```

Note: We have correlated variables in this data set. Random forests handle them fairly well.

Challenge: Try increasing `mtry` and `ntree`. Do the results improve?

Solution:

```{r eval=FALSE}
set.seed(4567)
wwfor <- randomForest(quality ~ ., data = whitewineR, mtry = 5)
print(wwfor)
```

```{r eval=FALSE}
set.seed(4567)
wwfor <- randomForest(quality ~ ., data = whitewineR, ntree = 1000)
print(wwfor)
```

Neither of the above changes has much of an effect.
